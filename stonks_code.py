import asyncio
import json
import re
from decimal import Decimal, InvalidOperation
from typing import List, Dict, Any

import aiohttp
from bs4 import BeautifulSoup

BASE_URL = "https://markets.businessinsider.com"
INDEX_URL = BASE_URL + "/index/components/s&p_500?p={}"
CBR_URL = "https://www.cbr.ru/scripts/XML_daily.asp"


async def fetch(session, url: str) -> str:
    async with session.get(url) as response:
        return await response.text()


async def get_usd_to_rub(session) -> Decimal:
    xml = await fetch(session, CBR_URL)
    soup = BeautifulSoup(xml, "xml")
    usd_rate = soup.find("Valute", {"ID": "R01235"}).Value.text.replace(
        ",", "."
    )
    print(f"üìä –¢–µ–∫—É—â–∏–π –∫—É—Ä—Å USD –∫ RUB: {usd_rate}")
    return Decimal(usd_rate)


def safe_decimal(value: str) -> Decimal:
    """–ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç —Å—Ç—Ä–æ–∫—É –≤ Decimal, –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—è –æ—à–∏–±–∫–∏."""
    try:
        return Decimal(value.replace(",", "").strip())
    except (InvalidOperation, AttributeError):
        return Decimal(0)


async def parse_company_page(
    session, url: str, usd_to_rub: Decimal
) -> Dict[str, Any]:
    """–ü–∞—Ä—Å–∏—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—É –∫–æ–º–ø–∞–Ω–∏–∏, –≤—ã—Ç–∞—Å–∫–∏–≤–∞–µ—Ç P/E, 52 Week Low/High."""
    html = await fetch(session, url)
    soup = BeautifulSoup(html, "lxml")

    name_code_section = soup.find("h1", class_="price-section__identifiers")
    if not name_code_section:
        print(f"‚ùó –ù–µ –Ω–∞–π–¥–µ–Ω –±–ª–æ–∫ —Å –Ω–∞–∑–≤–∞–Ω–∏–µ–º –∏ –∫–æ–¥–æ–º –∫–æ–º–ø–∞–Ω–∏–∏: {url}")
        return {}

    name = name_code_section.find(
        "span", class_="price-section__label"
    ).text.strip()
    code = name_code_section.find(
        "span", class_="price-section__category"
    ).text.strip()

    price_usd = safe_decimal(
        soup.find("span", class_="price-section__current-value").text
    )
    price_rub = price_usd * usd_to_rub

    pe_tag = soup.find(string=re.compile("P/E Ratio"))
    pe = (
        safe_decimal(pe_tag.find_next("span").text)
        if pe_tag
        else Decimal("inf")
    )

    # 52 Week Low / High
    try:
        week_low = safe_decimal(
            soup.find(string="52 Week Low").find_next("span").text
        )
        week_high = safe_decimal(
            soup.find(string="52 Week High").find_next("span").text
        )
        potential_profit = (
            ((week_high - week_low) / week_low) * 100
            if week_low and week_high
            else 0
        )
    except AttributeError:
        print(f"‚ö†Ô∏è –ù–µ–ø–æ–ª–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –æ 52 Week Low/High –¥–ª—è {name} ({code})")
        potential_profit = 0

    print(f"‚úÖ {name} ({code}) —É—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–∞")
    return {
        "name": name,
        "code": code,
        "price": float(price_rub),
        "P/E": float(pe),
        "potential_profit": float(potential_profit),
    }


async def parse_sp500():
    """–ü–∞—Ä—Å–∏—Ç –∏–Ω–¥–µ–∫—Å S&P 500 –ø–æ —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º."""
    async with aiohttp.ClientSession() as session:
        usd_to_rub = await get_usd_to_rub(session)
        companies = []

        for page in range(1, 3):
            url = INDEX_URL.format(page)
            index_page = await fetch(session, url)
            soup = BeautifulSoup(index_page, "lxml")

            table_rows = soup.select(
                "div.table-responsive table.table tbody tr"
            )
            print(f"üìä –°—Ç—Ä–∞–Ω–∏—Ü–∞ {page}: –Ω–∞–π–¥–µ–Ω–æ {len(table_rows)} –∫–æ–º–ø–∞–Ω–∏–π")

            for row in table_rows:
                cols = row.find_all("td")
                if len(cols) < 2:
                    continue  # –ü—Ä–æ–ø—É—Å–∫ –ø—É—Å—Ç—ã—Ö —Å—Ç—Ä–æ–∫

                company_link = BASE_URL + cols[0].find("a")["href"]
                print(f"üåê –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–æ–º–ø–∞–Ω–∏–∏: {company_link}")

                year_growth_text = (
                    cols[-1]
                    .text.strip()
                    .replace("\n", "")
                    .replace("%", "")
                    .replace(",", ".")
                )
                try:
                    year_growth = float(year_growth_text)
                except ValueError:
                    print(
                        f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Ä–æ—Å—Ç–∞ –∫–æ–º–ø–∞–Ω–∏–∏: '{year_growth_text}'"
                    )
                    year_growth = 0

                company_data = await parse_company_page(
                    session, company_link, usd_to_rub
                )
                if company_data:
                    company_data["growth"] = year_growth
                    companies.append(company_data)

        print(f"‚úÖ –í—Å–µ–≥–æ –∫–æ–º–ø–∞–Ω–∏–π —Å–æ–±—Ä–∞–Ω–æ: {len(companies)}")
        await save_top_10(companies)


def save_json(filename: str, data: List[Dict[str, Any]]):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç JSON-—Ñ–∞–π–ª."""
    print(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ {len(data)} –∑–∞–ø–∏—Å–µ–π –≤ —Ñ–∞–π–ª: {filename}")
    with open(filename, "w", encoding="utf-8") as file:
        json.dump(data, file, indent=4, ensure_ascii=False)


async def save_top_10(companies: List[Dict[str, Any]]):
    """–°–æ—Ä—Ç–∏—Ä—É–µ—Ç –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ç–æ–ø-10 –∫–æ–º–ø–∞–Ω–∏–π –ø–æ –∫—Ä–∏—Ç–µ—Ä–∏—è–º."""
    save_json(
        "top_10_price.json", sorted(companies, key=lambda x: -x["price"])[:10]
    )
    save_json("top_10_pe.json", sorted(companies, key=lambda x: x["P/E"])[:10])
    save_json(
        "top_10_growth.json",
        sorted(companies, key=lambda x: -x["growth"])[:10],
    )
    save_json(
        "top_10_potential_profit.json",
        sorted(companies, key=lambda x: -x["potential_profit"])[:10],
    )


if __name__ == "__main__":
    asyncio.run(parse_sp500())
    print("üéâ –í—Å–µ –¥–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ —Å–æ–±—Ä–∞–Ω—ã –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã.")
